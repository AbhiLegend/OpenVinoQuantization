{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNgcmzuL-79u"
      },
      "outputs": [],
      "source": [
        "# Install openvino package\n",
        "%pip install -q \"openvino>=2023.1.0\" nncf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Set the data and model directories\n",
        "DATA_DIR = Path(\"data\")\n",
        "MODEL_DIR = Path('model')\n",
        "model_repo = 'pytorch-cifar-models'\n",
        "\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "MODEL_DIR.mkdir(exist_ok=True)"
      ],
      "metadata": {
        "id": "UtRhm69mAahI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pprint import pprint\n",
        "pprint(torch.hub.list(\"chenyaofo/pytorch-cifar-models\", force_reload=True))"
      ],
      "metadata": {
        "id": "phv1NBPUAgV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "if not Path(model_repo).exists():\n",
        "    !git clone https://github.com/chenyaofo/pytorch-cifar-models.git\n",
        "\n",
        "sys.path.append(model_repo)"
      ],
      "metadata": {
        "id": "_uvyICS3Aobc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_cifar_models import cifar10_resnet32\n",
        "\n",
        "model = cifar10_resnet32(pretrained=True)"
      ],
      "metadata": {
        "id": "pQLvhVVJAwdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openvino as ov\n",
        "\n",
        "model.eval()\n",
        "\n",
        "ov_model = ov.convert_model(model, input=[(1,3,96,96)])\n",
        "\n",
        "ov.save_model(ov_model, MODEL_DIR / \"mobilenet_v2.xml\")\n"
      ],
      "metadata": {
        "id": "ftynSX2kA4hC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import STL10\n",
        "\n",
        "DATA_DIR = './data'  # Define your data directory here\n",
        "\n",
        "# Load STL10 dataset without normalization for computing mean and std\n",
        "dataset_for_stats = STL10(root=DATA_DIR, split='train', transform=transforms.ToTensor(), download=True)\n",
        "\n",
        "data_loader_for_stats = torch.utils.data.DataLoader(dataset_for_stats, batch_size=1000, shuffle=False, num_workers=4)\n",
        "\n",
        "mean = 0.0\n",
        "for images, _ in data_loader_for_stats:\n",
        "    batch_samples = images.size(0)  # batch size\n",
        "    images = images.view(batch_samples, images.size(1), -1)\n",
        "    mean += images.mean(2).sum(0)\n",
        "mean = mean / len(data_loader_for_stats.dataset)\n",
        "\n",
        "variance = 0.0\n",
        "for images, _ in data_loader_for_stats:\n",
        "    batch_samples = images.size(0)\n",
        "    images = images.view(batch_samples, images.size(1), -1)\n",
        "    variance += ((images - mean.unsqueeze(1))**2).sum([0, 2])\n",
        "std = torch.sqrt(variance / (len(data_loader_for_stats.dataset) * 96 * 96))\n",
        "\n",
        "# Now, use the computed mean and std values for normalization and set up the dataset and loader\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((mean[0], mean[1], mean[2]), (std[0], std[1], std[2]))\n",
        "])\n",
        "dataset = STL10(root=DATA_DIR, split='test', transform=transform, download=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "jNVl7MlmBAt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nncf\n",
        "\n",
        "def transform_fn(data_item):\n",
        "    image_tensor = data_item[0]\n",
        "    return image_tensor.numpy()\n",
        "\n",
        "quantization_dataset = nncf.Dataset(val_loader, transform_fn)"
      ],
      "metadata": {
        "id": "xR2hltaDB4mN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quant_ov_model = nncf.quantize(ov_model, quantization_dataset)"
      ],
      "metadata": {
        "id": "612uGyXZCCQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ov.save_model(quant_ov_model, MODEL_DIR / \"quantized_mobilenet_v2.xml\")"
      ],
      "metadata": {
        "id": "frbMtfYhGS5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "\n",
        "def test_accuracy(ov_model, data_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for (batch_imgs, batch_labels) in tqdm(data_loader):\n",
        "        result = ov_model(batch_imgs)[0]\n",
        "        top_label = np.argmax(result)\n",
        "        correct += top_label == batch_labels.numpy()\n",
        "        total += 1\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "XLbHaNmRODR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "\n",
        "core = ov.Core()\n",
        "device = widgets.Dropdown(\n",
        "    options=core.available_devices + [\"AUTO\"],\n",
        "    value='AUTO',\n",
        "    description='Device:',\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "device"
      ],
      "metadata": {
        "id": "J76fDUbMOYQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O4SWThTEZW9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "core = ov.Core()\n",
        "compiled_model = core.compile_model(ov_model, device.value)\n",
        "optimized_compiled_model = core.compile_model(quant_ov_model, device.value)\n",
        "\n",
        "orig_accuracy = test_accuracy(compiled_model, val_loader)\n",
        "optimized_accuracy = test_accuracy(optimized_compiled_model, val_loader)"
      ],
      "metadata": {
        "id": "-0r5QfrpO-cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference FP16 model (OpenVINO IR)\n",
        "!benchmark_app -m \"model/mobilenet_v2.xml\" -d $device.value -api async -t 15"
      ],
      "metadata": {
        "id": "4Hnf56mmZcn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference INT8 model (OpenVINO IR)\n",
        "!benchmark_app -m \"model/quantized_mobilenet_v2.xml\" -d $device.value -api async -t 15"
      ],
      "metadata": {
        "id": "S6Yglo8wZrja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision.datasets import STL10\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define all possible labels from the STL10 dataset\n",
        "labels_names = [\"airplane\", \"bird\", \"car\", \"cat\", \"deer\", \"dog\", \"horse\", \"monkey\", \"ship\", \"truck\"]\n",
        "\n",
        "DATA_DIR = './data'\n",
        "\n",
        "# Load the STL10 dataset and compute mean and std for normalization (assuming you've done this before)\n",
        "# ...\n",
        "\n",
        "# Create the dataset and data loader\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((mean[0], mean[1], mean[2]), (std[0], std[1], std[2]))\n",
        "])\n",
        "dataset = STL10(root=DATA_DIR, split='test', transform=transform, download=True)\n",
        "val_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "all_pictures, all_labels = [], []\n",
        "for img, lbl in val_loader:\n",
        "    all_pictures.append(img.squeeze().numpy())\n",
        "    all_labels.append(lbl.item())\n",
        "\n",
        "def plot_pictures(indexes: list):\n",
        "    images, label_texts = [], []\n",
        "    for idx in indexes:\n",
        "        assert idx < len(all_pictures), f'Cannot get index {idx}, there are only {len(all_pictures)} images'\n",
        "        pic = np.rollaxis(all_pictures[idx], 0, 3)\n",
        "        images.append(pic)\n",
        "        label_texts.append(labels_names[all_labels[idx]])\n",
        "\n",
        "    f, axarr = plt.subplots(1, 4)\n",
        "    for i, ax in enumerate(axarr):\n",
        "        ax.imshow(images[i])\n",
        "        ax.set_title(label_texts[i])\n",
        "    plt.show()\n",
        "\n",
        "def infer_on_pictures(model, indexes: list, all_pictures):\n",
        "    predicted_labels = []\n",
        "    for idx in indexes:\n",
        "        assert idx < len(all_pictures), f'Cannot get index {idx}, there are only {len(all_pictures)} images'\n",
        "        with torch.no_grad():\n",
        "            outputs = model(torch.tensor(all_pictures[idx]).unsqueeze(0))\n",
        "\n",
        "            # Directly extract the tensor (value) from the OVDict without relying on the key\n",
        "            result_np = list(outputs.values())[0]  # get the first value from the dictionary\n",
        "\n",
        "            # Convert the numpy array to a PyTorch tensor\n",
        "            result = torch.tensor(result_np)\n",
        "\n",
        "            predicted_label = labels_names[torch.argmax(result)]\n",
        "            predicted_labels.append(predicted_label)\n",
        "    return predicted_labels\n",
        "\n",
        "\n",
        "\n",
        "# Assuming you've defined and loaded `compiled_model` and `optimized_compiled_model`\n",
        "indexes_to_infer = [7, 12, 15, 20]  # To plot, specify 4 indexes.\n",
        "\n",
        "plot_pictures(indexes_to_infer)\n",
        "\n",
        "results_float = infer_on_pictures(compiled_model, indexes_to_infer, all_pictures)\n",
        "results_quanized = infer_on_pictures(optimized_compiled_model, indexes_to_infer, all_pictures)\n",
        "\n",
        "print(f\"Labels for picture from float model : {results_float}.\")\n",
        "print(f\"Labels for picture from quantized model : {results_quanized}.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "wg4b17-RPVCO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}